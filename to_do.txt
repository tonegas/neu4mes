General things to do / note:
Most important:
- model checkpoint
- closed-loop training:
  - try to execute part of the NN in feedforward with the whole window size (vector inputs), and execute only the RLS part in closed loop one sample at a time
  - shuffle false does not work in some cases, in closed loop
  - when using inputs (closed-loop) instead of states, different values of loss are computed during the training. They should be the same.

- Once the symbolic tracing works, we can do the following to speed-up the training:
  - model.exportPythonModel() --> this will export the model in a Python script and will do the tracing
  - model.importPythonModel() --> import the traced model
  - jit compile in Pytorch or other tricks to pre-compile the model and speed-up the training
- the full model of ARD takes a lot to be trained
- when exporting the json in a net_trained.json, the function torch.searchsorted(x_data, x, right=True) is exported with true instead of True
- closed loop training: 
  - run a profiler in CLion.
  - the training is extremely slow (w.r.t. Keras, for the same batch_size and predict_samples). 
  - try to trace the model 
  - investigate the warning "Creating a tensor from a list of numpy.ndarrays is extremely slow ..."
- The new MPLVisualizer does not work in Python
- shuffle_flag = True does not work or does not work as expected in closed-loop training
- the user should be able to give each layer (including FIR layers) a custom name, and the same names should appear in the json as well, to make it easier to identify them and load them in a new model as initial weights
- test with log_internal=True for the closed-loop training --> see the example test_recurrent_train.py
- the requirements.txt do not work on a new environment (windows), only the requirements_macos_x86.txt work
- compute the loss function in a consistent way after the training, in the resulting table printed by neu4mes
- when training with early stopping, the training loop should finally return the model with the best validation loss, and not the model of the last epoch. Or at least the user should be able to decide whether to return the model of the last epoch or the model with the best validation loss. (This could be for example defined within the early stopping method.) In the current version, it is not clear which model is returned at the end.
- read the csv data using the headers 
- model export and load: when exporting a model in a json, save the json with the model name, rather than with the current date and time.
- open loop training followed by closed loop training: define a state instead of an input and use prediction window = 0 to train the model in open loop
- method to print the trained NN weights
- method to plot the trained NN weights
- print the total number of trainable parameters in the model
- the option "range" in fuzzify must yield an error if it receives a list with more than 2 elements
- the framework is not repeatable, even if a random seed is set. This is due to the shuffling of the data during training. Investigate this.
- rewrite the function init_negexp: it should be centered around the present time step, and it should be decaying exponentially in the past and in the future.
- function to interpolate the imported datasets at a rate given by the user
- handle multiple datasets (or a single dataset that contains multiple independent datasets) when training in closed-loop

Jupyter Notebook:
- the MPLVisulizer does not seem to work in the notebook
- we get this warning:
/Users/mattiapiccinini/Documents/Research/Neu4Mes/tutorials/../neu4mes/model.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)
  self.all_parameters[name] = nn.Parameter(torch.tensor(param_data['values'], dtype=torch.float32), requires_grad=True)

Notes:
- I prepared the dataset with an additional column to indicate when a new dataset begins.
