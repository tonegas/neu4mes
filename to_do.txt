General things to do / note:
- try open loop evaluation of the mass estimator in keras and in neu4mes, to compare the CPU times
- the full model of ARD takes a lot to be trained
- when exporting the json in a net_trained.json, the function torch.searchsorted(x_data, x, right=True) is exported with true instead of True
- closed loop training: 
  - run a profiler in CLion.
  - the training is extremely slow (w.r.t. Keras, for the same batch_size and predict_samples). Try to trace the model and to investigate the warning "Creating a tensor from a list of numpy.ndarrays is extremely slow ..."
  - with the loss (MSE) seems to be proportional to the batch size. Is the loss multiplied by the batch size? 
  - when using inputs instead of states, different values of loss are computed during the training. They should be the same.
  - if predict_samples covers the whole dataset, and batch_size = 1, then there is only one batch with predict_samples samples.
    if instead batch_size = 5, then there should be again only one batch but it should contain 5*predict_samples samples, i.e., the
    training should last longer. Instead, it seems that the training lasts the same.
- The new MPLVisualizer does not work in Python
- shuffle_flag = True does not work or does not work as expected in closed-loop training
- the symbolic tracer does not work
- the user should be able to give each layer (including FIR layers) a custom name, and the same names should appear in the json as well, to make it easier to identify them and load them in a new model as initial weights
- test with log_internal=True for the closed-loop training --> see the example test_recurrent_train.py
- the requirements.txt do not work on a new environment (windows), only the requirements_macos_x86.txt work
- compute the loss function in a consistent way after the training, in the resulting table printed by neu4mes
- when training with early stopping, the training loop should finally return the model with the best validation loss, and not the model of the last epoch. Or at least the user should be able to decide whether to return the model of the last epoch or the model with the best validation loss. (This could be for example defined within the early stopping method.) In the current version, it is not clear which model is returned at the end.
- read the csv data using the headers 
- model export and load: when exporting a model in a json, save the json with the model name, rather than with the current date and time.
- open loop training followed by closed loop training: define a state instead of an input and use prediction window = 0 to train the model in open loop
- method to print the trained NN weights
- method to plot the trained NN weights
- print the total number of trainable parameters in the model
- the option "range" in fuzzify must yield an error if it receives a list with more than 2 elements
- the framework is not repeatable, even if a random seed is set. This is due to the shuffling of the data during training. Investigate this.
- rewrite the function init_negexp: it should be centered around the present time step, and it should be decaying exponentially in the past and in the future.
- function to interpolate the imported datasets at a rate given by the user
- handle multiple datasets (or a single dataset that contains multiple independent datasets) when training in closed-loop

Jupyter Notebook:
- the MPLVisulizer does not seem to work in the notebook
- we get this warning:
/Users/mattiapiccinini/Documents/Research/Neu4Mes/tutorials/../neu4mes/model.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)
  self.all_parameters[name] = nn.Parameter(torch.tensor(param_data['values'], dtype=torch.float32), requires_grad=True)

Notes:
- I prepared the dataset with an additional column to indicate when a new dataset begins.